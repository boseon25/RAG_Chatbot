import requests
from langchain_community.vectorstores import FAISS

from config import INDEX_DIR, OLLAMA_API, MODEL_NAME
from embedder import BGEEmbedding
from embedder import emb_wrapper

# Ollama-Exaone í˜¸ì¶œ í•¨ìˆ˜
# Ollama-Exaone í˜¸ì¶œ í•¨ìˆ˜
import re
import requests
from config import OLLAMA_API, MODEL_NAME

def ask_ollama(prompt: str, model: str = MODEL_NAME, max_tokens: int = 512):
    payload = {
        "model":   model,
        "prompt":  prompt,
        "stream":  False,
        "options": {"num_predict": max_tokens}
    }

    try:
        res = requests.post(OLLAMA_API, json=payload, timeout=120)
        if res.ok:
            raw_response = res.json().get("response", "").strip()

            # ğŸ”§ í›„ì²˜ë¦¬: íŠ¹ìˆ˜ë¬¸ì ì œê±° (ì´ëª¨ì§€, ì „í™”ê¸° ê¸°í˜¸ ë“±)
            cleaned_response = re.sub(r"[â˜ğŸ“ğŸ“±]", "", raw_response)

            return cleaned_response
        else:
            return f"Ollama ì˜¤ë¥˜: {res.status_code} {res.text}"
    except Exception as e:
        return f"ìš”ì²­ ì‹¤íŒ¨: {e}"


# RAG íŒŒì´í”„ë¼ì¸
def ask_rag(query: str, top_k:int = 3):
    # ê²€ìƒ‰
    db   = FAISS.load_local(INDEX_DIR, emb_wrapper,
                            allow_dangerous_deserialization=True)
    docs = db.similarity_search(query, k=top_k)
    context = "\n\n".join(d.page_content[:800] for d in docs)   # ê³¼ë„í•œ ê¸¸ì´ ë°©ì§€

    # ì§€ì‹œì–´ê°€ ìˆëŠ” í”„ë¡¬í”„íŠ¸
    prompt = f"""ì•„ë˜ [ë¬¸ì„œ] ë‚´ìš©ì— ê·¼ê±°í•´ì„œë§Œ í•œêµ­ì–´ë¡œ ë‹µí•˜ë¼.
ë¬¸ì„œì— ì •ë³´ê°€ ì—†ìœ¼ë©´ 'ëª¨ë¥´ê² ìŠµë‹ˆë‹¤'ë¼ê³  ë‹µí•˜ë¼.

[ë¬¸ì„œ]
{context}

[ì§ˆë¬¸]
{query}

[ë‹µë³€]"""

    # LLM í˜¸ì¶œ
    return ask_ollama(prompt)