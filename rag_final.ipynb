{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "    BGE-M3 + FAISS + Ollama-Exaone3.5  â€• ì™„ì „ ë¡œì»¬ RAG íŒŒì´í”„ë¼ì¸\n",
        "   Â· PDFë¥¼ ë²¡í„°í™”í•´ FAISS ì¸ë±ìŠ¤ì— ëˆ„ì  ì €ì¥\n",
        "   Â· ê²€ìƒ‰(FAISS) â†’ ìƒì„±(Ollama)ê¹Œì§€ í•œ ë²ˆì— ìˆ˜í–‰\n",
        "   Â· í”„ë¡¬í”„íŠ¸ì— â€˜ë¬¸ì„œ ê·¼ê±°ë§Œ ë‹µí•˜ë¼, ì—†ìœ¼ë©´ ëª¨ë¥´ê² ë‹¤â€™ ê·œì¹™ì„ ë„£ì–´ í• ë£¨ì‹œë„¤ì´ì…˜ ìµœì†Œí™”\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "tCfdqRzNAwzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ollama\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "!ollama --version\n",
        "!nohup ollama serve > log.txt 2>&1 &\n",
        "!ollama pull exaone3.5:2.4b\n",
        "!curl http://localhost:11434/api/tags"
      ],
      "metadata": {
        "id": "lQDq3cvqAJe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 0) í•„ìˆ˜ íŒ¨í‚¤ì§€\n",
        "# (ë…¸íŠ¸ë¶ì´ë©´ ì•ì— !, ë¡œì»¬ì‰˜ì´ë©´ pip install â€¦)\n",
        "!pip install -qU langchain-community langchain-text-splitters \\\n",
        "                faiss-cpu FlagEmbedding pymupdf requests"
      ],
      "metadata": {
        "id": "ZPKtLdUhAP8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) í™˜ê²½ ì„¤ì •\n",
        "from pathlib import Path\n",
        "import numpy as np, requests\n",
        "from FlagEmbedding import FlagModel\n",
        "from langchain.embeddings.base import Embeddings\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS"
      ],
      "metadata": {
        "id": "PmjkJyBPAZFq"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PDF_PATH   = \"/content/drive/MyDrive/Colab Notebooks/pdf/2022ê°œì •êµìœ¡ê³¼ì •ììœ í•™ê¸°ìš´ì˜ì•ˆë‚´ì„œ.pdf\"\n",
        "INDEX_DIR  = \"faiss_bge_m3_index\"\n",
        "CHUNK_SIZE = 800      # ğŸ‘ˆ ê¸¸ì´ë¥¼ ì¤„ì—¬ ì „í™”ë²ˆí˜¸ ë“±ì´ ì•ˆ ì˜ë¦¬ë„ë¡\n",
        "CHUNK_OVER = 100\n",
        "OLLAMA_API = \"http://localhost:11434/api/generate\"\n",
        "MODEL_NAME = \"exaone3.5:2.4b\""
      ],
      "metadata": {
        "id": "uIaCRNWqAZ_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  2) BGE-M3 ì„ë² ë”© ë˜í¼\n",
        "class BGEEmbedding(Embeddings):\n",
        "    \"\"\"FlagEmbedding â†’ LangChain compatible wrapper\"\"\"\n",
        "    def __init__(self, model_name=\"BAAI/bge-m3\", fp16=True):\n",
        "        self.model = FlagModel(model_name, use_fp16=fp16)\n",
        "    def _encode(self, texts):\n",
        "        vecs = self.model.encode(texts, batch_size=32, max_length=8192)\n",
        "        vecs = np.asarray(vecs, dtype=\"float32\")\n",
        "        vecs /= np.linalg.norm(vecs, axis=1, keepdims=True) + 1e-12\n",
        "        return vecs\n",
        "    def embed_documents(self, texts):  return self._encode(texts)\n",
        "    def embed_query(self, text):       return self._encode([text])[0]\n",
        "\n",
        "emb_wrapper = BGEEmbedding(fp16=True)      # GPU ì—†ìœ¼ë©´ fp16=False"
      ],
      "metadata": {
        "id": "Jx6ZzwuSAehu"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) ì¸ë±ìŠ¤ ìƒì„± / ì—…ë°ì´íŠ¸\n",
        "def build_faiss_index():\n",
        "    loader = PyMuPDFLoader(PDF_PATH)\n",
        "    docs   = loader.load()                              # page ë‹¨ìœ„\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVER\n",
        "    )\n",
        "    chunks  = splitter.split_documents(docs)\n",
        "    if Path(INDEX_DIR).exists():\n",
        "        db = FAISS.load_local(INDEX_DIR, emb_wrapper,\n",
        "                              allow_dangerous_deserialization=True)\n",
        "        db.add_documents(chunks)\n",
        "    else:\n",
        "        db = FAISS.from_documents(chunks, emb_wrapper)\n",
        "    db.save_local(INDEX_DIR)\n",
        "    print(f\"âœ… ì¸ë±ìŠ¤ ì™„ë£Œ / ì´ ë²¡í„°: {db.index.ntotal:,}\")\n",
        "    return db"
      ],
      "metadata": {
        "id": "J2dNB7BEAhR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) Ollama-Exaone í˜¸ì¶œ í•¨ìˆ˜\n",
        "def ask_ollama(prompt: str, model: str = MODEL_NAME, max_tokens:int = 512):\n",
        "    payload = {\n",
        "        \"model\":   model,\n",
        "        \"prompt\":  prompt,\n",
        "        \"stream\":  False,\n",
        "        \"options\": {\"num_predict\": max_tokens}   # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ í™•ë³´\n",
        "    }\n",
        "    res = requests.post(OLLAMA_API, json=payload, timeout=120)\n",
        "    if res.ok:\n",
        "        return res.json().get(\"response\", \"\").strip()\n",
        "    return f\"âŒ Ollama ì˜¤ë¥˜: {res.status_code} {res.text}\""
      ],
      "metadata": {
        "id": "idHgjwz-Ai-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) RAG íŒŒì´í”„ë¼ì¸\n",
        "def ask_rag(query: str, top_k:int = 3):\n",
        "    # (1) ê²€ìƒ‰\n",
        "    db   = FAISS.load_local(INDEX_DIR, emb_wrapper,\n",
        "                            allow_dangerous_deserialization=True)\n",
        "    docs = db.similarity_search(query, k=top_k)\n",
        "    context = \"\\n\\n\".join(d.page_content[:800] for d in docs)   # ê³¼ë„í•œ ê¸¸ì´ ë°©ì§€\n",
        "\n",
        "    # (2) ì§€ì‹œì–´ê°€ ìˆëŠ” í”„ë¡¬í”„íŠ¸\n",
        "    prompt = f\"\"\"ì•„ë˜ [ë¬¸ì„œ] ë‚´ìš©ì— ê·¼ê±°í•´ì„œë§Œ í•œêµ­ì–´ë¡œ ë‹µí•˜ë¼.\n",
        "ë¬¸ì„œì— ì •ë³´ê°€ ì—†ìœ¼ë©´ 'ëª¨ë¥´ê² ìŠµë‹ˆë‹¤'ë¼ê³  ë‹µí•˜ë¼.\n",
        "\n",
        "[ë¬¸ì„œ]\n",
        "{context}\n",
        "\n",
        "[ì§ˆë¬¸]\n",
        "{query}\n",
        "\n",
        "[ë‹µë³€]\"\"\"\n",
        "\n",
        "    # (3) LLM í˜¸ì¶œ\n",
        "    return ask_ollama(prompt)"
      ],
      "metadata": {
        "id": "GoovduNMAnpT"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6) ì‹¤í–‰ ì˜ˆì‹œ\n",
        "if __name__ == \"__main__\":\n",
        "    #build_faiss_index()                                  # â˜… ìµœì´ˆ 1íšŒ\n",
        "    q = \"í•™êµì•ˆì „ì‚¬ê³  ì˜ˆë°© Â·ê´€ë¦¬ ì²´ê³„ì˜ ê³ ë„í™” ê³¼ì œì— ëŒ€í•œ ë‹´ë‹¹ë¶€ì„œëŠ” ì–´ë””ê³  ì—°ë½ì²˜ê°€ ì–´ë–»ê²Œ ë¼?\"\n",
        "    print(\"Q:\", q)\n",
        "    print(\"A:\", ask_rag(q))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J69ZtU45Ao_I",
        "outputId": "e56739f3-451e-4d21-c065-339d3a3eae2a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: í•™êµì•ˆì „ì‚¬ê³  ì˜ˆë°© Â·ê´€ë¦¬ ì²´ê³„ì˜ ê³ ë„í™” ê³¼ì œì— ëŒ€í•œ ë‹´ë‹¹ë¶€ì„œëŠ” ì–´ë””ê³  ì—°ë½ì²˜ê°€ ì–´ë–»ê²Œ ë¼?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A: í•™êµì•ˆì „ì‚¬ê³  ì˜ˆë°©Â·ê´€ë¦¬ ì²´ê³„ì˜ ê³ ë„í™” ê³¼ì œì— ëŒ€í•œ ë‹´ë‹¹ë¶€ì„œëŠ” **í•™êµì•ˆì „ê³¼**ì´ë©°, ì—°ë½ì²˜ëŠ” **239-0841**ì…ë‹ˆë‹¤.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}