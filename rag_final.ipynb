{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "    BGE-M3 + FAISS + Ollama-Exaone3.5  ― 완전 로컬 RAG 파이프라인\n",
        "   · PDF를 벡터화해 FAISS 인덱스에 누적 저장\n",
        "   · 검색(FAISS) → 생성(Ollama)까지 한 번에 수행\n",
        "   · 프롬프트에 ‘문서 근거만 답하라, 없으면 모르겠다’ 규칙을 넣어 할루시네이션 최소화\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "tCfdqRzNAwzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ollama\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "!ollama --version\n",
        "!nohup ollama serve > log.txt 2>&1 &\n",
        "!ollama pull exaone3.5:2.4b\n",
        "!curl http://localhost:11434/api/tags"
      ],
      "metadata": {
        "id": "lQDq3cvqAJe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 0) 필수 패키지\n",
        "# (노트북이면 앞에 !, 로컬쉘이면 pip install …)\n",
        "!pip install -qU langchain-community langchain-text-splitters \\\n",
        "                faiss-cpu FlagEmbedding pymupdf requests"
      ],
      "metadata": {
        "id": "ZPKtLdUhAP8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) 환경 설정\n",
        "from pathlib import Path\n",
        "import numpy as np, requests\n",
        "from FlagEmbedding import FlagModel\n",
        "from langchain.embeddings.base import Embeddings\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS"
      ],
      "metadata": {
        "id": "PmjkJyBPAZFq"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PDF_PATH   = \"/content/drive/MyDrive/Colab Notebooks/pdf/2022개정교육과정자유학기운영안내서.pdf\"\n",
        "INDEX_DIR  = \"faiss_bge_m3_index\"\n",
        "CHUNK_SIZE = 800      # 👈 길이를 줄여 전화번호 등이 안 잘리도록\n",
        "CHUNK_OVER = 100\n",
        "OLLAMA_API = \"http://localhost:11434/api/generate\"\n",
        "MODEL_NAME = \"exaone3.5:2.4b\""
      ],
      "metadata": {
        "id": "uIaCRNWqAZ_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  2) BGE-M3 임베딩 래퍼\n",
        "class BGEEmbedding(Embeddings):\n",
        "    \"\"\"FlagEmbedding → LangChain compatible wrapper\"\"\"\n",
        "    def __init__(self, model_name=\"BAAI/bge-m3\", fp16=True):\n",
        "        self.model = FlagModel(model_name, use_fp16=fp16)\n",
        "    def _encode(self, texts):\n",
        "        vecs = self.model.encode(texts, batch_size=32, max_length=8192)\n",
        "        vecs = np.asarray(vecs, dtype=\"float32\")\n",
        "        vecs /= np.linalg.norm(vecs, axis=1, keepdims=True) + 1e-12\n",
        "        return vecs\n",
        "    def embed_documents(self, texts):  return self._encode(texts)\n",
        "    def embed_query(self, text):       return self._encode([text])[0]\n",
        "\n",
        "emb_wrapper = BGEEmbedding(fp16=True)      # GPU 없으면 fp16=False"
      ],
      "metadata": {
        "id": "Jx6ZzwuSAehu"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) 인덱스 생성 / 업데이트\n",
        "def build_faiss_index():\n",
        "    loader = PyMuPDFLoader(PDF_PATH)\n",
        "    docs   = loader.load()                              # page 단위\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVER\n",
        "    )\n",
        "    chunks  = splitter.split_documents(docs)\n",
        "    if Path(INDEX_DIR).exists():\n",
        "        db = FAISS.load_local(INDEX_DIR, emb_wrapper,\n",
        "                              allow_dangerous_deserialization=True)\n",
        "        db.add_documents(chunks)\n",
        "    else:\n",
        "        db = FAISS.from_documents(chunks, emb_wrapper)\n",
        "    db.save_local(INDEX_DIR)\n",
        "    print(f\"✅ 인덱스 완료 / 총 벡터: {db.index.ntotal:,}\")\n",
        "    return db"
      ],
      "metadata": {
        "id": "J2dNB7BEAhR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) Ollama-Exaone 호출 함수\n",
        "def ask_ollama(prompt: str, model: str = MODEL_NAME, max_tokens:int = 512):\n",
        "    payload = {\n",
        "        \"model\":   model,\n",
        "        \"prompt\":  prompt,\n",
        "        \"stream\":  False,\n",
        "        \"options\": {\"num_predict\": max_tokens}   # 컨텍스트 길이 확보\n",
        "    }\n",
        "    res = requests.post(OLLAMA_API, json=payload, timeout=120)\n",
        "    if res.ok:\n",
        "        return res.json().get(\"response\", \"\").strip()\n",
        "    return f\"❌ Ollama 오류: {res.status_code} {res.text}\""
      ],
      "metadata": {
        "id": "idHgjwz-Ai-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) RAG 파이프라인\n",
        "def ask_rag(query: str, top_k:int = 3):\n",
        "    # (1) 검색\n",
        "    db   = FAISS.load_local(INDEX_DIR, emb_wrapper,\n",
        "                            allow_dangerous_deserialization=True)\n",
        "    docs = db.similarity_search(query, k=top_k)\n",
        "    context = \"\\n\\n\".join(d.page_content[:800] for d in docs)   # 과도한 길이 방지\n",
        "\n",
        "    # (2) 지시어가 있는 프롬프트\n",
        "    prompt = f\"\"\"아래 [문서] 내용에 근거해서만 한국어로 답하라.\n",
        "문서에 정보가 없으면 '모르겠습니다'라고 답하라.\n",
        "\n",
        "[문서]\n",
        "{context}\n",
        "\n",
        "[질문]\n",
        "{query}\n",
        "\n",
        "[답변]\"\"\"\n",
        "\n",
        "    # (3) LLM 호출\n",
        "    return ask_ollama(prompt)"
      ],
      "metadata": {
        "id": "GoovduNMAnpT"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6) 실행 예시\n",
        "if __name__ == \"__main__\":\n",
        "    #build_faiss_index()                                  # ★ 최초 1회\n",
        "    q = \"학교안전사고 예방 ·관리 체계의 고도화 과제에 대한 담당부서는 어디고 연락처가 어떻게 돼?\"\n",
        "    print(\"Q:\", q)\n",
        "    print(\"A:\", ask_rag(q))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J69ZtU45Ao_I",
        "outputId": "e56739f3-451e-4d21-c065-339d3a3eae2a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: 학교안전사고 예방 ·관리 체계의 고도화 과제에 대한 담당부서는 어디고 연락처가 어떻게 돼?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A: 학교안전사고 예방·관리 체계의 고도화 과제에 대한 담당부서는 **학교안전과**이며, 연락처는 **239-0841**입니다.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}